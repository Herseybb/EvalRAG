# EvalRAG

**Embedding Model Evaluation for Retrieval-Augmented Generation (RAG)**

## ðŸ“Œ Overview
This project aims to benchmark different text embedding models within a **Retrieval-Augmented Generation (RAG)** framework.  
By comparing retrieval and answer generation performance across multiple datasets, the project provides insights into the strengths and weaknesses of various embeddings in real-world QA scenarios.
This is a **self-learning project** designed to explore the research process in NLP.

## ðŸŽ¯ Objectives
- Build a reproducible RAG evaluation framework  
- Compare multiple embedding models (e.g., `all-MiniLM-L6-v2`, `multi-qa-mpnet-base-dot-v1`)  
- Evaluate performance on standard datasets 
- Metrics: Recall@k, MRR, Answer F1, Human evaluation (TBD)

## ðŸ“‚ Project Structure (TBD)
EvalRAG/
â”‚â”€â”€ data/ # Datasets
â”‚â”€â”€ notebooks/ # Experiment notebooks
â”‚â”€â”€ README.md # Project documentation


